RTM.Norm <- function (data,mu,Sigma,delta=0,cutoff,truncation, conf.level = 0.95)
{
#Checking the conditions
p <- length(mu)
if (!all(dim(Sigma) == c(p, p)))
stop("incompatible arguments")
eS <- eigen(Sigma, symmetric = TRUE)
ev <- eS$values
if (!all(ev >= -1e-06 * abs(ev[1L])))
stop("'Sigma' is not positive definite")
if (nrow(data)<3)
stop("The observations must be at least three")
choices <- c("left","right")
alt <- pmatch(truncation, choices)
truncation <- choices[alt]
if (length(truncation) > 1 || is.na(truncation))
stop("truncation must be one \"left\", \"right\"")
if (!missing(conf.level))
if (length(conf.level) != 1 || is.na(conf.level) ||
conf.level < 0 || conf.level > 1)
stop("conf.level must be a number between 0 and 1")
if (truncation=="left") {
mux<-mu[1]
muy<-mu[2]
sigx<-diag(Sigma)[1]
sigy<-diag(Sigma)[2]
cor<-(Sigma[2,1])/sqrt(sigx*sigy)
z<-(mu[1]-cutoff)/sqrt(sigx)
SigXY<-(dnorm(z)/pnorm(z))*(z-dnorm(z)/pnorm(z))*(sqrt(sigx)-
cor*sqrt(sigy))^2+sigx+sigy-2*cor*sqrt(sigx*sigy)
RTM<-(sqrt(sigx)-sqrt(sigy)*cor)*dnorm(z)/pnorm(z)
RTM<-round(RTM,4)
#extract pre and post variables
X<-data[,1]
Y<-data[,2]
#compute the treatment effect
delta.est<-round(mean(X-Y+RTM),4)
#test the treatment effect by eliminating RTM
test<- z.test(X-Y+RTM,sigma.x =SigXY,mu=delta,conf.level=conf.level)
}else{
mux<-mu[1]
muy<-mu[2]
sigx<-diag(Sigma)[1]
sigy<-diag(Sigma)[2]
cor<-(Sigma[2,1])/sqrt(sigx*sigy)
z<-(cutoff-mu[1])/sqrt(sigx)
SigXY<-(dnorm(z)/(1-pnorm(z)))*(z-dnorm(z)/(1-pnorm(z)))*(sqrt(sigx)-
cor*sqrt(sigy))^2+sigx+sigy-2*cor*sqrt(sigx*sigy)
RTM<-(sqrt(sigx)-sqrt(sigy)*cor)*dnorm(z)/(1-pnorm(z))
RTM<-round(RTM,4)
#extract pre and post variables
X<-data[,1]
Y<-data[,2]
#compute the treatment effect
delta.est<-round(mean(X-Y-RTM),4)
#test the treatment effect by eliminating RTM
test<- z.test(X-Y-RTM,sigma.x =SigXY,mu=delta,conf.level=conf.level)
}
#confidence interval
cint<- test$conf.int
#extract p-value, test statistics
pval<- test$p.value
z.stat<-test$statistic
estimates<-c(RTM,delta.est)
names(z.stat)<-"z"
names(estimates)<-c("RTM","Delta")
method<-"***TREATMENT MEANS UNDER REGRESSION TO THE MEAN***"
mu<-delta
names(mu)<-"treatment effect"
dname <- deparse(substitute(data))
output<-list(statistic = z.stat, p.value = pval,alternative="two.sided",
estimate = estimates, null.value = mu, conf.int = cint,
method = method,data.name = dname)
attr(output, "class") <- "htest"
return(output)
}
RTM.Norm(treatGrp,mu=mu,Sigma = varcov,cutoff = cutoff,truncation = "right")
mean(treatGrp[,1]-treatGrp[,2])
#extract treatment group, baseline and followup measurement
treatGrp<- LeadExposedChildren[LeadExposedChildren$Group=="A",c(3,6)]
treatGrp
#extract treatment group, baseline and followup measurement
treatGrp<- LeadExposedChildren[LeadExposedChildren$Group=="A",c(3,6)]
cutoff<-19.5
mu<-c(19.808,18.168)
varcov<-matrix(c(8.269^2,0.885*8.269*7.929,0.885*8.269*7.929,7.929^2),nrow = 2)
RTM.Norm(treatGrp,mu=mu,Sigma = varcov,cutoff = cutoff,truncation = "right")
mean(treatGrp[,1]-treatGrp[,2])
nll=function(pars, data) {
# Extract parameters from the vector
n=50
mu = pars[1]
sigm = pars[2]
tr=19.5 ### This is the cut-off point and may be different for different data
# Calculate Negative Log-LIkelihood
n*log(sigm)+(n*varx1+n*(xbar1-mu)^2)/2/sigm^2+n*log(pnorm(tr,mu,sigm))
}
tr=19.5
x1=treatGrp[,1]
x2=treatGrp[,2]
xbar1=mean(x1);xbar1
xbar2=mean(x2);xbar2
varx1=var(x1);varx2=var(x2)
r=cor(x1,x2);r
B=cov(x1,x2)/varx1
mle = optim(par = c(xbar1,sqrt(varx1)), fn = nll,
control = list(parscale = c(mu = 5.5, sigm = .3)))
mux=mle$par[1];sigmaX=mle$par[2];mux;sigmaX
z=(tr-mux)/sigmaX;z
mux
tr
mux
pars[1]
z=(tr-mux)/sigmaX;z
alpa=xbar2;
sigma=sqrt(varx2)*sqrt(1-r^2);
muy=alpa+B*(mux-xbar1);
sigmaY=sqrt(sigma^2+sigmaX^2*B^2);
rh=sigmaX*B/sqrt(sigma^2+sigmaX^2*B^2);
delt=mux-muy;
delt
RTM=(sigmaX-sigmaY*rh)*dnorm(z)/pnorm(z)
RTM
RTM=(sigmaX-sigmaY*rh)*dnorm(z)/(1-pnorm(z))
RTM
delt=mux-muy;
RTM=(sigmaX-sigmaY*rh)*dnorm(z)/(1-pnorm(z))
mux;muy;sigmaX;sigmaY;rh
delt;RTM;
### estimate the RTM effect
RTM<-RTM.Norm(treatGrp,mu=mu,Sigma = varcov,cutoff = cutoff,truncation = "right")
RTM
### Total effect
mean(treatGrp[,1]-treatGrp[,2])
### treatment effect
RTM$estimate[2]
### RTM
RTM$estimate[1]
devtools::document()
devtools::document()
library(RTM)
devtools::load_all(".")
rm(list = c("RTM.Norm"))
devtools::load_all(".")
getwd()
use_r("bv-t.R")
library(devtools)
use_r("bv-t.R")
library(RTM)
getwd()
remove.packages(RTM)
remove.packages("RTM")
#install.packages("devtools")
library(devtools)
install_github("umair1nonly/RTM")
library(RTM)
help("RTM.Norm")
library(RTM)
help(dbvt)
help(dnorm)
?dbvt()
devtools::load_all(".")
library(RTM)
help("RTM.Norm")
help(glm)
help("RTM.Norm")
help(dbvt)
library(RTM)
devtools::load_all(".")
devtools::load_all(".")
library(RTM)
help(glm)
library(BSDA)
help("z.test")
library(RTM)
devtools::load_all(".")
devtools::load_all(".")
library(RTM)
help("glm")
remove.packages(RTM)
library(RTM)
remove.packages(RTM)
remove.packages("RTM")
#install.packages("devtools")
library(devtools)
install_github("umair1nonly/RTM")
library(RTM)
help("RTM.Norm")
help(dbvt)
data("LeadExposedChildren")
### extract treatment group, baseline and followup measurement
treatGrp<- LeadExposedChildren[LeadExposedChildren$Group=="A",c(3,6)]
### cutoff point it is different for different studies
cutoff<-19.5
### define mean and variance covariance matrix
mu<-c(19.808,18.168)
varcov<-matrix(c(8.269^2,0.885*8.269*7.929,
0.885*8.269*7.929,7.929^2),nrow = 2)
### estimate the RTM effect
RTM<-RTM.Norm(treatGrp,mu=mu,Sigma = varcov,cutoff = cutoff,truncation = "right")
RTM
### Total effect
mean(treatGrp[,1]-treatGrp[,2])
### treatment effect
RTM$estimate[2]
### RTM
RTM$estimate[1]
citation("RTM")
#install.packages("BAS")
library("BAS")
# enumeration with default method="BAS"
pima.cch = bas.glm(type ~ ., data=Pima.tr, n.models= 2^7,
method="BAS",
betaprior=CCH(a=1, b=532/2, s=0), family=binomial(),
modelprior=beta.binomial(1,1))
library(readxl)
locations <- read_excel("C:/Users/HAIER/Downloads/locations.xlsx")
View(locations)
names(locations)
# enumeration with default method="BAS"
pima.cch = bas.glm(CVSet ~ ., data=locations[,-1], n.models= 2^7,
method="BAS",
betaprior=CCH(a=1, b=532/2, s=0), poisson(link = "log"),
modelprior=beta.binomial(1,1))
# enumeration with default method="BAS"
pima.cch = bas.glm(cvSet ~ ., data=locations[,-1], n.models= 2^7,
method="BAS",
betaprior=CCH(a=1, b=532/2, s=0), poisson(link = "log"),
modelprior=beta.binomial(1,1))
summary(pima.cch)
image(pima.cch)
# Note MCMC.iterations are set to 2500 for illustration purposes due to time
# limitations for running examples on CRAN servers.
# Please check convergence diagnostics and run longer in practice
pima.robust = bas.glm(cvSet ~ ., data=locations[,-1], n.models= 2^7,
method="MCMC", MCMC.iterations=2500,
betaprior=robust(), family=binomial(),
modelprior=beta.binomial(1,1))
# Note MCMC.iterations are set to 2500 for illustration purposes due to time
# limitations for running examples on CRAN servers.
# Please check convergence diagnostics and run longer in practice
pima.robust = bas.glm(cvSet ~ ., data=locations[,-1], n.models= 2^7,
method="MCMC", MCMC.iterations=2500,
betaprior=robust(), family=poisson(link = "log"),
modelprior=beta.binomial(1,1))
pima.BIC = bas.glm(cvSet ~ ., data=locations[,-1], n.models= 2^7,
method="BAS+MCMC", MCMC.iterations=2500,
betaprior=bic.prior(), family=poisson(link = "log"),
modelprior=uniform())
# Poisson example
if(requireNamespace("glmbb", quietly=TRUE)) {
data(crabs, package='glmbb')
#short run for illustration
crabs.bas = bas.glm(cvSet ~ ., data=locations[,-1],
family=poisson(),
betaprior=EB.local(), modelprior=uniform(),
method='MCMC', n.models=2^10, MCMC.iterations=2500,
prob.rw=.95)
}
crabs.bas
summary(crabs.bas)
library(devtools)
library(RTM)
use_version()
library(RTM)
devtools::load_all(".")
version(RTM)
help(RTM)
library(RTM)
help(RTM)
devtools::load_all(".")
library(RTM)
library(RTM)
help(RTM)
help("RTM.T")
library(RTM)
devtools::load_all(".")
library(RTM)
help(RTM)
library(RTM)
library(RTM)
help(RTM)
library(RTM)
devtools::load_all(".")
citation(RTM)
library(RTM)
citation(RTM)
help(RTM)
help(RTM.norm)
use_version()
use_version()
library(RTM)
library(BSDA)
help(BSDA)
detach("package:BSDA", unload = TRUE)
library(BSDA)
cite(RTM)
citation(RTM)
citation("RTM")
devtools::load_all(".")
devtools::load_all(".")
library(RTM)
library(RTM)
library(RTM)
help("RTM.T")
citation("RTM")
debug(family())
debug(family(binomial()))
debug(family(binomial))
debug(binomial())
debug(binomial)
debug(binomial)
debug(binomial)
debug(z.test)
debug(z.test)
debug(z.test())
debug(binomial())
family()
poisson()
debug(poisson())
debug(poisson)
debug(poisson)
# Load the fda package for functional data analysis
library(fda)
# Load the fda package for functional data analysis
install.packages("fda")
# Load the fda package for functional data analysis
#install.packages("fda")
library(fda)
# Simulate some functional data with 100 observations and 101 equally spaced points
n <- 100 # number of observations
m <- 101 # number of points
t <- seq(0, 1, length.out = m) # time grid
X <- matrix(rnorm(n * m), nrow = n, ncol = m) # random matrix
X
X <- matrix(rnorm(n * m), nrow = n, ncol = m) # random matrix
basis <- create.fourier.basis(c(0, 1), nbasis = 11) # Fourier basis
fd <- smooth.basis(t, X, fdPar(basis))$fd # smooth the data to get functional objects
help(farma.fit)
help(farma)
??farma()
# Plot the functional data
plot(fd)
# Fit a FARMA(1,1) model to the functional data using the farma function
farma.fit <- farma(fd, p = 1, q = 1)
# Plot the functional data
plot(fd)
# Fit a FARMA(1,1) model to the functional data using the farma function
farma.fit <- farma(fd, p = 1, q = 1)
# Install and load the necessary package
install.packages("forecast")
library(forecast)
# Simulate some time series data
set.seed(123)
# Simulate some time series data
set.seed(123)
n <- 100
time <- 1:n
simulated_data <- 0.5 * sin(2 * pi * time / 12) + rnorm(n)
# Convert data to a time series object
ts_data <- ts(simulated_data)
# Fit a FARMA model
arma_order <- c(1, 1)  # AR order = 1, MA order = 1
far_order <- 3         # Functional order
fitted_model <- farima(ts_data, order = arma_order, seasonal = list(order = c(0, 0, 0), period = 12), functional = TRUE, farorder = far_order)
library(tseries)
# Simulate some time series data
set.seed(123)
n <- 100
time <- 1:n
simulated_data <- 0.5 * sin(2 * pi * time / 12) + rnorm(n)
# Convert data to a time series object
ts_data <- ts(simulated_data)
# Fit an ARFIMA model
arfima_order <- c(1, 0, 1)  # AR order = 1, I(d) = 0 (non-fractional differencing), MA order = 1
fitted_model <- arfima(ts_data, order = arfima_order)
mydata <- read.csv("C:/Users/HAIER/Downloads/mydata.csv")
View(mydata)
temp_data=mydata$temp
temp_data
length(temp_data)
a= is.na(temp_data)
a
### This is daily hourly data of 5 years in which first four year is
### training set
dailycurves= matrix(temp_data[1:43824], nrow=1826 ,ncol=24, byrow=TRUE)
head(dailycurves)
dailyhumidy<-matrix(mydata$humidity[1:43824], nrow=1826 ,ncol=24, byrow=TRUE)
#install.packages("far")
library(far)
#install.packages("far")
library(far)
install.packages("far")
#install.packages("far")
library(far)
xfd<- as.fdata(y,col=1,p=24,name = "temp")
i=1
xin<- dailycurves[1:c(1459+i),]
xin
# str(dailycurves)
#xin=dailycurves
nr<- nrow(xin)
nc<- ncol(xin)
n = nr*nc
y <- matrix(t(xin),n,1)
y
xin
mydata <- read_csv("D:/My thesis/temp isb data/mydata.csv")
temp_data=mydata$temp
length(temp_data)
a= is.na(temp_data)
a
### This is daily hourly data of 5 years in which first four year is
### training set
#install.packages("far")
library(far)
xfd<- as.fdata(a,col=1,p=24,name = "temp")
xfd
plot(fd)
class(fd)
class(xfd)
k1<- far.cv(xfd,y="temp",ncv=30,na.rm = FALSE)$minL2[1]
k1
k1
far1<- far(xfd,kn=k1,na.rm = FALSE)
far1
plot(far1)
y
library(tseries)
library(TSA)
library(forecast)
library(fda)
library(fda.usc)
library(readr)
library(far)
mydata <- read.csv("C:/Users/HAIER/Downloads/mydata.csv")
View(mydata)
temp_data=mydata$temp
length(temp_data)
a= is.na(temp_data)
### Convert to functional data and build functional auto regressive (FAR)
xfd<- as.fdata(y,col=1,p=24,name = "temp")
### Convert to functional data and build functional auto regressive (FAR)
xfd<- as.fdata(temp_data,col=1,p=24,name = "temp")
k1<- far.cv(xfd,y="temp",ncv=30,na.rm = FALSE)
far1<- far(xfd,kn=k1,na.rm = FALSE)
k1<- far.cv(xfd,y="temp",ncv=30,na.rm = FALSE)
far1<- far(xfd,kn=k1,na.rm = FALSE)
k1
k1<- far.cv(xfd,y="temp",ncv=30,na.rm = FALSE)$minL2[1]
far1<- far(xfd,kn=k1,na.rm = FALSE)
far1
plot(far1)
forcastmat<- predict(far1,newdata = temp_data,na.rm = FALSE)$temp
forcastmat<- predict(far1,newdata = temp_data,na.rm = FALSE)$temp
forcastmat<- predict(far1,newdata = xfd,na.rm = FALSE)$temp
forcastmat
plot(forcastmat)
length(forcastmat)
forcastmat
```r
```r
RTM
rbinom(10,10,prob = 0.5)
x= rbinom(10,10,prob = 0.5)
hist(x)
x= rbinom(10,10,prob = 0.5)
hist(x)
x= rbinom(100,10,prob = 0.5)
hist(x)
x= rbinom(100,10,prob = 0.5)
hist(x)
x= rbinom(10000,10,prob = 0.5)
hist(x)
dbinom(5,size = 10,prob = 0.5)
# Generate a 10 random numbers
rbinom(10,10,0.5)
# Generate a 10 random numbers
x=rbinom(10,10,0.5)
dbinom(x,size = 10,prob = 0.5)
y=dbinom(x,size = 10,prob = 0.5)
plot(x,y)
plot(x,y,type = "l")
# Generate a 10 random numbers
x=sort(rbinom(10,10,0.5))
x
# Generate a 10 random numbers
x=sort(rbinom(10,10,0.5))
y=dbinom(x,size = 10,prob = 0.5)
plot(x,y,type = "l")
> w <- 4
w <- 4
n <- 10
alpha <- 0.05
pi.hat <- w/n
pi.hat
var.wald <- pi.hat*(1 - pi.hat)/n
var.wald
qnorm(p = 1- alpha /2)
w <- 4
lower <- pi.hat - qnorm(p = 1- alpha /2) * sqrt( var. wald )
pi.hat <- w/n
var.wald <- pi.hat*(1 - pi.hat)/n
lower <- pi.hat - qnorm(p = 1- alpha /2) * sqrt( var. wald )
lower <- pi.hat - qnorm(p = 1- alpha /2) * sqrt( var.wald )
lower
upper <- pi.hat + qnorm(p = 1- alpha /2) * sqrt( var.wald )
upper
round(data.frame(lower , upper ), 4)
prop.test(x=4,n=10)
library(RTM)
library(RTM)
